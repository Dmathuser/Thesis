% Encoding: UTF-8

@Article{Rein:VIME,
  author     = {Rein Houthooft and Xi Chen and Yan Duan and John Schulman and Filip De Turck and Pieter Abbeel},
  title      = {Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks},
  journal    = {CoRR},
  year       = {2016},
  volume     = {abs/1605.09674},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/HouthooftCDSTA16.bib},
  eprint     = {1605.09674},
  eprinttype = {arXiv},
  timestamp  = {Mon, 03 Sep 2018 12:15:29 +0200},
  url        = {http://arxiv.org/abs/1605.09674},
}

@Article{DBLP:journals/corr/abs-2007-13442,
  author     = {Pierre M{\'{e}}nard and Omar Darwiche Domingues and Anders Jonsson and Emilie Kaufmann and Edouard Leurent and Michal Valko},
  title      = {Fast active learning for pure exploration in reinforcement learning},
  journal    = {CoRR},
  year       = {2020},
  volume     = {abs/2007.13442},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2007-13442.bib},
  eprint     = {2007.13442},
  eprinttype = {arXiv},
  timestamp  = {Wed, 24 Jan 2024 16:32:51 +0100},
  url        = {https://arxiv.org/abs/2007.13442},
}

@Article{DBLP:journals/corr/abs-2008-04388,
  author     = {Grgur Kovac and Adrien Laversanne{-}Finot and Pierre{-}Yves Oudeyer},
  title      = {{GRIMGEP:} Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning},
  journal    = {CoRR},
  year       = {2020},
  volume     = {abs/2008.04388},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2008-04388.bib},
  eprint     = {2008.04388},
  eprinttype = {arXiv},
  timestamp  = {Fri, 14 Aug 2020 15:14:45 +0200},
  url        = {https://arxiv.org/abs/2008.04388},
}

@Article{DBLP:journals/corr/abs-1908-06976,
  author     = {Arthur Aubret and La{\"{e}}titia Matignon and Salima Hassas},
  title      = {A survey on intrinsic motivation in reinforcement learning},
  journal    = {CoRR},
  year       = {2019},
  volume     = {abs/1908.06976},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1908-06976.bib},
  eprint     = {1908.06976},
  eprinttype = {arXiv},
  timestamp  = {Mon, 26 Aug 2019 13:20:40 +0200},
  url        = {http://arxiv.org/abs/1908.06976},
}

@Book{Sutton1998,
  title     = {Reinforcement Learning: An Introduction},
  publisher = {The MIT Press},
  year      = {2018},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  edition   = {Second},
  added-at  = {2019-07-13T10:11:53.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  timestamp = {2019-07-13T10:11:53.000+0200},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
}

@Article{DBLP:journals/corr/TangHFSCDSTA16,
  author     = {Haoran Tang and Rein Houthooft and Davis Foote and Adam Stooke and Xi Chen and Yan Duan and John Schulman and Filip De Turck and Pieter Abbeel},
  title      = {{\#}Exploration: {A} Study of Count-Based Exploration for Deep Reinforcement Learning},
  journal    = {CoRR},
  year       = {2016},
  volume     = {abs/1611.04717},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/TangHFSCDSTA16.bib},
  eprint     = {1611.04717},
  eprinttype = {arXiv},
  timestamp  = {Mon, 03 Sep 2018 12:15:29 +0200},
  url        = {http://arxiv.org/abs/1611.04717},
}

@Article{10.3389/fncir.2013.00037,
  author   = {Daniel Y. Little and Friedrich T. Sommer},
  title    = {Learning and exploration in action-perception loops},
  journal  = {Frontiers in Neural Circuits},
  year     = {2013},
  volume   = {7},
  issn     = {1662-5110},
  abstract = {Discovering the structure underlying observed data is a recurring problem in machine learning with important applications in neuroscience. It is also a primary function of the brain. When data can be actively collected in the context of a closed action-perception loop, behavior becomes a critical determinant of learning efficiency. Psychologists studying exploration and curiosity in humans and animals have long argued that learning itself is a primary motivator of behavior. However, the theoretical basis of learning-driven behavior is not well understood. Previous computational studies of behavior have largely focused on the control problem of maximizing acquisition of rewards and have treated learning the structure of data as a secondary objective. Here, we study exploration in the absence of external reward feedback. Instead, we take the quality of an agent's learned internal model to be the primary objective. In a simple probabilistic framework, we derive a Bayesian estimate for the amount of information about the environment an agent can expect to receive by taking an action, a measure we term the predicted information gain (PIG). We develop exploration strategies that approximately maximize PIG. One strategy based on value-iteration consistently learns faster than previously developed reward-free exploration strategies across a diverse range of environments. Psychologists believe the evolutionary advantage of learning-driven exploration lies in the generalized utility of an accurate internal model. Consistent with this hypothesis, we demonstrate that agents which learn more efficiently during exploration are later better able to accomplish a range of goal-directed tasks. We will conclude by discussing how our work elucidates the explorative behaviors of animals and humans, its relationship to other computational models of behavior, and its potential application to experimental design, such as in closed-loop neurophysiology studies.},
  doi      = {10.3389/fncir.2013.00037},
  url      = {https://www.frontiersin.org/articles/10.3389/fncir.2013.00037},
}

@Unpublished{TPIG:github,
  author = {David Mathews and Pyeatt, Larry D.},
  title  = {https://github.com/Dmathuser/Thesis},
  note   = {Git Repository for the "Temporal PIG: Improving PIG to Resist the Noisy-TV Problem" Thesis Paper},
  url    = {https://github.com/Dmathuser/Thesis},
}

@Article{Ladosz_2022,
  author    = {Ladosz, Pawel and Weng, Lilian and Kim, Minwoo and Oh, Hyondong},
  title     = {Exploration in deep reinforcement learning: A survey},
  journal   = {Information Fusion},
  year      = {2022},
  volume    = {85},
  pages     = {1–22},
  month     = sep,
  issn      = {1566-2535},
  doi       = {10.1016/j.inffus.2022.03.003},
  publisher = {Elsevier BV},
  url       = {http://dx.doi.org/10.1016/j.inffus.2022.03.003},
}

@Article{FToCFaIM:Jurgen,
  author   = {Schmidhuber, Jürgen},
  title    = {Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)},
  journal  = {IEEE Transactions on Autonomous Mental Development},
  year     = {2010},
  volume   = {2},
  number   = {3},
  pages    = {230-247},
  doi      = {10.1109/TAMD.2010.2056368},
  keywords = {Art;Psychology;Pediatrics;Predictive models;Data compression;Computational intelligence;Intelligent robots;Feedback;Eyes;Fingers;Active learning;aesthetics theory;art;attention;developmental psychology;formal theory of creativity;fun;humor;limited computational resources;music;novel patterns;novelty;science;surprise;typology of intrinsic motivation},
}

@Comment{jabref-meta: databaseType:bibtex;}
